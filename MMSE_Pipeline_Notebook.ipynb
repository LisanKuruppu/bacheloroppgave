{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8187f971",
   "metadata": {},
   "source": [
    "### 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7f6914d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import Logistic_bootstrap_metrics as lbm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "import statsmodels.api as sm\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9a001",
   "metadata": {},
   "source": [
    "### 2. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5343dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/TrainTest_Table.csv\"\n",
    "train_test_df = pd.read_csv(file_path)\n",
    "\n",
    "train_df = train_test_df[train_test_df[\"Split\"] == \"Train\"]\n",
    "test_df = train_test_df[train_test_df[\"Split\"] == \"Test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e268e334",
   "metadata": {},
   "source": [
    "### 3. Define MMSE Question Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27308224",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmse_questions = {\n",
    "    \"MMYEAR\": \"What year is it?\",\n",
    "    \"MMMONTH\": \"What month is it?\",\n",
    "    \"MMDAY\": \"What day of the week is it?\",\n",
    "    \"MMSEASON\": \"What season is it?\",\n",
    "    \"MMDATE\": \"What is todayâ€™s date?\",\n",
    "    \"MMSTATE\": \"What state are we in?\",\n",
    "    \"MMCITY\": \"What city are we in?\",\n",
    "    \"MMAREA\": \"What county are we in?\",\n",
    "    \"MMHOSPIT\": \"What building are we in?\",\n",
    "    \"MMFLOOR\": \"What floor are we on?\",\n",
    "    \"WORD1\": \"Repeat the word.\",\n",
    "    \"WORD2\": \"Repeat the word.\",\n",
    "    \"WORD3\": \"Repeat the word.\",\n",
    "    \"MMD\": \"What is 100 minus 7?\",\n",
    "    \"MML\": \"What is the next number after subtracting 7?\",\n",
    "    \"MMR\": \"What is the next number after subtracting 7?\",\n",
    "    \"MMO\": \"What is the next number after subtracting 7?\",\n",
    "    \"MMW\": \"What is the next number after subtracting 7?\",\n",
    "    \"WORD1DL\": \"Can you recall the first word from earlier?\",\n",
    "    \"WORD2DL\": \"Can you recall the second word from earlier?\",\n",
    "    \"WORD3DL\": \"Can you recall the third word from earlier?\",\n",
    "    \"MMWATCH\": \"What is this object? (Watch)\",\n",
    "    \"MMPENCIL\": \"What is this object? (Pencil)\",\n",
    "    \"MMREPEAT\": \"Repeat after me: 'No ifs, ands, or buts.'\",\n",
    "    \"MMHAND\": \"Take this paper in your right hand.\",\n",
    "    \"MMFOLD\": \"Fold this paper in half.\",\n",
    "    \"MMONFLR\": \"Place the paper on the floor.\",\n",
    "    \"MMREAD\": \"Please read the sentence: 'Close your eyes.'\",\n",
    "    \"MMWRITE\": \"Please write a sentence.\",\n",
    "    \"MMDRAW\": \"Please copy this drawing.\"\n",
    "}\n",
    "\n",
    "mmse_context = {\n",
    "    \"MMYEAR\": \"Assesses awareness of the current year, which is often impaired in early AD.\",\n",
    "    \"MMMONTH\": \"Assesses awareness of the current month, which is often impaired in early AD.\",\n",
    "    \"MMDAY\": \"Assesses awareness of the day of the week, which is often impaired in early AD.\",\n",
    "    \"MMSEASON\": \"Assesses awareness of the current season, which is often impaired in early AD.\",\n",
    "    \"MMDATE\": \"Assesses the ability to identify today's date, which is often impaired in early AD.\",\n",
    "    \"MMSTATE\": \"Tests orientation to state location, which may be affected in later stages of AD.\",\n",
    "    \"MMCITY\": \"Tests orientation to city or town, which may be affected in later stages of AD.\",\n",
    "    \"MMAREA\": \"Tests orientation to the county or area, which may be affected in later stages of AD.\",\n",
    "    \"MMHOSPIT\": \"Tests awareness of the current building, which may be affected in later stages of AD.\",\n",
    "    \"MMFLOOR\": \"Tests the ability to identify the floor level, which may be affected in later stages of AD.\",\n",
    "    \"WORD1\": \"Tests immediate memory by repeating a presented word. Most AD patients perform well here initially.\",\n",
    "    \"WORD2\": \"Tests immediate memory by repeating a presented word. Most AD patients perform well here initially.\",\n",
    "    \"WORD3\": \"Tests immediate memory by repeating a presented word. Most AD patients perform well here initially.\",\n",
    "    \"MMD\": \"Tests sustained attention using subtraction, a task often impaired in AD.\",\n",
    "    \"MML\": \"Continues testing attention with serial 7s, a task often impaired in AD.\",\n",
    "    \"MMR\": \"Continues testing attention with serial 7s, a task often impaired in AD.\",\n",
    "    \"MMO\": \"Continues testing attention with serial 7s, a task often impaired in AD.\",\n",
    "    \"MMW\": \"Continues testing attention with serial 7s, a task often impaired in AD.\",\n",
    "    \"WORD1DL\": \"Tests delayed recall of a previously presented word. Impaired recall is an early hallmark of AD.\",\n",
    "    \"WORD2DL\": \"Tests delayed recall of a previously presented word. Impaired recall is an early hallmark of AD.\",\n",
    "    \"WORD3DL\": \"Tests delayed recall of a previously presented word. Impaired recall is an early hallmark of AD.\",\n",
    "    \"MMWATCH\": \"Tests ability to name a common object (watch), often impaired in moderate AD.\",\n",
    "    \"MMPENCIL\": \"Tests ability to name a common object (pencil), often impaired in moderate AD.\",\n",
    "    \"MMREPEAT\": \"Tests ability to repeat a complex sentence. Errors are common in moderate AD.\",\n",
    "    \"MMHAND\": \"Tests comprehension of a verbal command, often impaired in later stages of AD.\",\n",
    "    \"MMFOLD\": \"Tests comprehension of a verbal command, often impaired in later stages of AD.\",\n",
    "    \"MMONFLR\": \"Tests comprehension of a verbal command, often impaired in later stages of AD.\",\n",
    "    \"MMREAD\": \"Tests ability to read and understand a sentence. Impairment may reflect executive dysfunction in AD.\",\n",
    "    \"MMWRITE\": \"Tests ability to write a meaningful sentence. Impairment may reflect executive dysfunction in AD.\",\n",
    "    \"MMDRAW\": \"Tests visuospatial ability by copying a design. Impairments are common in later stages of AD.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3a662a",
   "metadata": {},
   "source": [
    "### 4. Generate MMSE Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97d0f196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_structured_mmse_prompts(df, mmse_questions):\n",
    "    prompts = []\n",
    "    for _, row in df.iterrows():\n",
    "        subject_id = row[\"subject_id\"]\n",
    "        visit = row[\"visit\"]\n",
    "        ad = row[\"AD\"] \n",
    "        for mmse_var, question in mmse_questions.items():\n",
    "            if mmse_var in df.columns:\n",
    "                score = row[mmse_var]\n",
    "                if not pd.isna(score):\n",
    "                    prompt = {\n",
    "                        \"subject_id\": subject_id,\n",
    "                        \"visit\": visit,\n",
    "                        \"AD\": ad,  # Updated key name\n",
    "                        \"MMSE Prompt\": (\n",
    "                            f\"Question: {question}\\n\"\n",
    "                            f\"Result: {1 if score == 1 else 0}\\n\"\n",
    "                        )\n",
    "                    }\n",
    "                    prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "\n",
    "def generate_contextual_mmse_prompts(df, mmse_questions, mmse_context):\n",
    "    prompts = []\n",
    "    for _, row in df.iterrows():\n",
    "        subject_id = row[\"subject_id\"]\n",
    "        visit = row[\"visit\"]\n",
    "        ad = row[\"AD\"]\n",
    "        for mmse_var, question in mmse_questions.items():\n",
    "            if mmse_var in df.columns:\n",
    "                score = row[mmse_var]\n",
    "                if not pd.isna(score):\n",
    "                    context = mmse_context.get(mmse_var, \"No context available.\")\n",
    "                    prompt = {\n",
    "                        \"subject_id\": subject_id,\n",
    "                        \"visit\": visit,\n",
    "                        \"AD\": ad,\n",
    "                        \"MMSE Prompt\": (\n",
    "                            f\"Question: {question}\\n\"\n",
    "                            f\"Context: {context}\\n\"\n",
    "                            f\"Result: {1 if score == 1 else 0}\\n\"\n",
    "                        )\n",
    "                    }\n",
    "                    prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "# Generate prompts \n",
    "structured_mmse_prompts_train = generate_structured_mmse_prompts(train_df, mmse_questions)\n",
    "structured_mmse_prompts_test = generate_structured_mmse_prompts(test_df, mmse_questions)\n",
    "\n",
    "# Generate contextual prompts\n",
    "contextual_mmse_prompts_train = generate_contextual_mmse_prompts(train_df, mmse_questions, mmse_context)\n",
    "contextual_mmse_prompts_test = generate_contextual_mmse_prompts(test_df, mmse_questions, mmse_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe88bed4",
   "metadata": {},
   "source": [
    "### 5. Save Prompts to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cac6337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df_prompts_train = pd.DataFrame(structured_mmse_prompts_train)\n",
    "df_prompts_test = pd.DataFrame(structured_mmse_prompts_test)\n",
    "\n",
    "df_context_train = pd.DataFrame(contextual_mmse_prompts_train)\n",
    "df_context_test = pd.DataFrame(contextual_mmse_prompts_test)\n",
    "\n",
    "# Save to CSV\n",
    "df_prompts_train.to_csv(\"data/MMSE_Prompts_Train.csv\", index=False)\n",
    "df_prompts_test.to_csv(\"data/MMSE_Prompts_Test.csv\", index=False)\n",
    "\n",
    "df_context_train.to_csv(\"data/MMSE_Context_Prompts_Train.csv\", index=False)\n",
    "df_context_test.to_csv(\"data/MMSE_Context_Prompts_Test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ebe68",
   "metadata": {},
   "source": [
    "### 6. Load Bio_ClinicalBERT and Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cfd2050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to extract CLS token embeddings\n",
    "def extract_cls_embedding(texts, model, tokenizer):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    return cls_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34018459",
   "metadata": {},
   "source": [
    "### 7. Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "902e3ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts and embeddings have been generated and saved to CSV files.\n"
     ]
    }
   ],
   "source": [
    "# Promts without context\n",
    "train_prompts = df_prompts_train[\"MMSE Prompt\"].tolist()\n",
    "test_prompts = df_prompts_test[\"MMSE Prompt\"].tolist()\n",
    "\n",
    "# Embedding Promts without context\n",
    "train_embeddings = extract_cls_embedding(train_prompts, model, tokenizer)\n",
    "test_embeddings = extract_cls_embedding(test_prompts, model, tokenizer)\n",
    "\n",
    "# Convert embeddings to DataFrames\n",
    "train_embeddings_df = pd.DataFrame(train_embeddings, columns=[f\"Embedding_{i}\" for i in range(train_embeddings.shape[1])])\n",
    "test_embeddings_df = pd.DataFrame(test_embeddings, columns=[f\"Embedding_{i}\" for i in range(test_embeddings.shape[1])])\n",
    "\n",
    "# Concatenate the embeddings DataFrame with the original DataFrame\n",
    "df_prompts_train = pd.concat([df_prompts_train.reset_index(drop=True), train_embeddings_df], axis=1)\n",
    "df_prompts_test = pd.concat([df_prompts_test.reset_index(drop=True), test_embeddings_df], axis=1)\n",
    "\n",
    "# Save the updated DataFrames to CSV\n",
    "df_prompts_train.to_csv(\"data/MMSE_Prompts_Train.csv\", index=False)\n",
    "df_prompts_test.to_csv(\"data/MMSE_Prompts_Test.csv\", index=False)\n",
    "\n",
    "# Print when the process is complete\n",
    "print(\"Prompts and embeddings have been generated and saved to CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96f0fe9",
   "metadata": {},
   "source": [
    "### 8. Extract Contextual Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a74f9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual prompts and embeddings have been generated and saved to CSV files.\n"
     ]
    }
   ],
   "source": [
    "# Promts with context\n",
    "train_context_prompts = df_context_train[\"MMSE Prompt\"].tolist()\n",
    "test_context_prompts = df_context_test[\"MMSE Prompt\"].tolist()\n",
    "\n",
    "# Embedding Promts with context\n",
    "train_context_embeddings = extract_cls_embedding(train_context_prompts, model, tokenizer)\n",
    "test_context_embeddings = extract_cls_embedding(test_context_prompts, model, tokenizer)\n",
    "\n",
    "# Convert embeddings to DataFrames\n",
    "train_context_embeddings_df = pd.DataFrame(train_context_embeddings, columns=[f\"Context_Embedding_{i}\" for i in range(train_context_embeddings.shape[1])])\n",
    "test_context_embeddings_df = pd.DataFrame(test_context_embeddings, columns=[f\"Context_Embedding_{i}\" for i in range(test_context_embeddings.shape[1])])\n",
    "\n",
    "# Concatenate the embeddings DataFrame with the original DataFrame\n",
    "df_context_train = pd.concat([df_context_train.reset_index(drop=True), train_context_embeddings_df], axis=1)\n",
    "df_context_test = pd.concat([df_context_test.reset_index(drop=True), test_context_embeddings_df], axis=1)\n",
    "\n",
    "# Save the updated DataFrames to CSV\n",
    "df_context_train.to_csv(\"data/MMSE_Context_Prompts_Train.csv\", index=False)\n",
    "df_context_test.to_csv(\"data/MMSE_Context_Prompts_Test.csv\", index=False)\n",
    "\n",
    "# Print when the process is complete\n",
    "print(\"Contextual prompts and embeddings have been generated and saved to CSV files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
